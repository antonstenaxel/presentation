<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <link rel="stylesheet" href="presentation.css" />
    <link
      href="https://fonts.googleapis.com/css?family=Lato"
      rel="stylesheet"
    />
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <title>Presentation</title>
  </head>
  <body>
    <div class="container">
      <section>
        <h1>The Data</h1>
        <p>
          In total, 297 074 news articles has been gathered from
          <a href="https://www.avanza.se/placera/forstasidan.html">Placera</a>
          between 2010-01-01 and 2019-03-19, and 120 799 comments from
          <a href="https://shareville.se/activity/comments">Shareville</a>
          between . Stock prices has been gathered between 2014-03-24 and
          2019-03-20
        </p>
        <p>
          Stock prices for 675 stock in OMX Stockholm and First North were
          gathered from
          <a href="http://www.nasdaqomxnordic.com/shares/historicalprices"
            >Nasaq</a
          >
          between 2010-01-01 and 2019-03-19. Using an online lookup for swedish
          holidays, each data point is associated with the opening date-time and
          the previous closing date-time, adjusted for weekends and holidays.
          Furthermore, to keep datapoints independent, values of previous day
          was added. It should be noted that these values are not adjusted for splits. An example of datapoints can be seen below:
          <img src="table.png" />
        </p>
      </section>

      <section>
        <h1>Preprocessing</h1>
        <h2>Extracting the time</h2>
        <p>
          First step of preprocessing has been to extract date-times from the
          news articles, which was straight forward as the times were part of
          the scraped html code. Below is the resulting hourly distribution.

          <iframe
            width="900"
            height="400"
            frameborder="0"
            scrolling="no"
            src="//plot.ly/~antonstenaxel/53.embed"
          ></iframe>
        </p>

        <h2>Processing the Articles</h2>
        <p>
          Next the titles and bodies of each article was separated using the structure of the underlying html. Stop words
          were removed based on the python library nltk's list of 114 swedish
          stopwords. The text was split into sentences using the previously
          mentioned library's method 
          <a href="https://www.nltk.org/api/nltk.tokenize.html"
            ><span class="code">sent_tokenize</span></a
          > Then each sentence was turned into word using <a href="https://www.nltk.org/api/nltk.tokenize.html"
            ><span class="code">word_tokenize</span></a>
          . Each article was thus turned into a list of sentences which in turn
          is a list of words, which were all converted to lower case. These
          words were then subject to filtering using regular expressions. First
          any website adress was filered out by</br>
          <span class="code">r'.*\.(com|se|net|org)\$'</span></br>
          followed by some commonly occuring files</br>
          <span class="code">r'.*\.(pdf|aspx|doc|txt)$'</span></br>
          Punctuation and digits were also filtered out and finaly any letter not present in the swedish alphabet (such as dashes used for dividers etc. ) was filtered out. 
        </p>
        <p>
          Finally the text was lemmatized. The "snowball-stemmer" found in the nltk-library did not seem to work very well for swedish, a lot of words were wrongly stemmed. Instead, a list of the words in the the so far preprocessed corpus occuring a minimum of 20 times were extracted, which turned out to be 70 617 unique words. A script was set up using SAOL's (Svenska Akademins OrdLista) API and for each word the lemma was saved if it could be uniqely decided and it exsited. This worked out quite well, as 10 021 lemmas for 19 200 words could be assigned (a lot of the original 70 000 words were proper nouns etc, which have no lemma).
        </p>

        <p>In summation,</p> 
          <p class="example">
          I Oasmia Pharmaceutical AB (publ) (“Oasmia” eller ”Bolaget”) avhölls i dag extra bolagsstämma. Den extra bolagsstämman var sammankallad på begäran av aktieägare representerande mer än 10 procent av aktierna i Bolaget för att behandla frågan om val av styrelse för Bolaget.</p>

          <p>was converted to </p>
       <p class="example">
        [['oasmia', 'pharmaceutical', 'ab', 'publ', 'oasmia', 'bolag', 'avhålla', 'dag', 'extra', 'bolagsstämma'], ['extra', 'bolagsstämma', 'sammankallad', 'begäran', 'aktieägare', 'representera', 'mer', 'procent', 'aktie', 'bolag', 'behandla', 'fråga', 'val', 'styrelse', 'bolag']]
      </p>
        

      </section>
      <section>
        <h1>Representation</h1>
        <h2>Word2Vec</h2>
        <p>To represent the words numerically, a derivation of word2vec called <a href="https://arxiv.org/pdf/1607.04606.pdf">FastText</a> was implemented. In addition to word contexts it includes information about subword n-grams, which in theory should make it more resilient to morphological variation. Each word is represented by a 100-dimensional vector. For visialisation purposes, the resulting 54 484 word-vectors were projected to a two-dimensional space using t-SNE, and the resulting (quite large, it might take some time to load) plot is visible <a href="./word2vec.html" target="_blank">here</a>. The results seem quite promosing, for isntance one can zoom in on a cluster and find that three words very close to each other are "analytikerprognosen", "analytikerprognoserna" and "analytikerprognoserna". That these three are basically the same (made up) word would have been hard to catch using methods such as bag-of-words. Another interesting feature is that these are quite close to the english words "analytical" and "analytics". One can also find the words closest in the sense of geometric distance. For example, the closest words to "vinstvarning" are "vinstvarna", "rapportbesvikelse", "rapportbesvikelser", "rapportbesvikelsen", "varningsflagg", "rapportsläppet", "varningstext", "prognossänkningar", "prognossäkningen" and "kvartalsrapportering". </p>

        <p>Performing a singular value decomposition of the wordvectors and plotting the cummulative sum of the singular values we can see that the typical threshold of 90% of singular values are reached at around 84 dimensions. Perhaps one can even try more dimensions. </p>
        <iframe width="800" height="400" frameborder="0" scrolling="no" src="//plot.ly/~antonstenaxel/61.embed"></iframe>

        <h2>Bag-of-Words</h2>
        <p>
        I created a script that transforms the corpus to bag-of-words vectors. Additionally it calculated tf-idf and optionally the LDA. It only keeps the 1000 most common words (1% of unique words) to make it manageable, which still retains 66 % of the total number of words.

        For instance </p>
        <p class="example">['Stockholm',
          'direkt',
          'notering',
          'nasdaq',
          'omx',
          'Stockholm',
          'cap',
          'lista',
          'bolag',
          'styrelse',
          'bedöma',
          'notering',
          'komma',
          'äga',
          'första',
          'halvår',
          'framgå',
          'pressmeddelande',
          'dag',
          'lista',
          'first',
          'north',
          'börsredaktionen',
          'nyhetsbyrå',
          'direkt']</p>
          <p> is turned into the BOW vector</p>
          <p class="example">[(0, 2),
            (8, 1),
            (9, 2),
            (21, 1),
            (55, 1),
            (68, 1),
            (92, 2),
            (101, 1),
            (153, 1),
            (155, 1),
            (168, 1),
            (176, 1),
            (180, 1),
            (194, 1),
            (196, 2),
            (197, 1),
            (204, 1),
            (208, 1),
            (209, 1),
            (210, 1),
            (211, 1)]</p>
            <p>which is turned to the if-idf vector </p>
            <p class="example">[(0, 0.044471656715664214),
              (8, 0.14658607992381323),
              (9, 0.04771443001151147),
              (21, 0.06071004029378088),
              (55, 0.05134262091706973),
              (68, 0.16048204650484016),
              (92, 0.4127350984578276),
              (101, 0.16847477254760929),
              (153, 0.21641683430576114),
              (155, 0.06794781460114734),
              (168, 0.09107905959708487),
              (176, 0.19027452656466873),
              (180, 0.29432102709761115),
              (194, 0.14826542830176798),
              (196, 0.6051661283540677),
              (197, 0.2331440461025338),
              (204, 0.17233668737470673),
              (208, 0.2143897847583252),
              (209, 0.11955196670723481),
              (210, 0.10938687638386194),
              (211, 0.12800056230445672)]</p>
      </section>
      <p>In addition to this, a LDA was trained using (arbitrarily) 100 topics, characterized by a distribution of words. Some of the topics are </p>
      <p class="example">0.154*"fingerprint" + 0.128*"cards" + 0.087*"placera" + 0.024*"fpc" + 0.023*"lansering" + 0.018*"stod" + 0.017*"använda" + 0.016*"inleda" + 0.015*"kort" + 0.015*"framgå"</p>
      <p class="example">0.154*"bud" + 0.141*"bostad" + 0.033*"stödja" + 0.029*"carnegie" + 0.024*"lägga" + 0.021*"rekommendera" + 0.018*"inleda" + 0.016*"projekt" + 0.015*"produktion" + 0.013*"kontant"</p>
      <section>
        <h1>Model</h1>
        <h2>Data-set</h2>
        <p>
          To build the Data-set a method <span class="code">generate_datapoints</span> was created. It generates samples where there is at least one news article present as well as a set of criteria based on two arguments:</p>
          <ul>
            <li>inter-day change:   $\frac{\text{open}(d)}{\text{close}(d-1)} -1$</li>
            <li>the trading value, which is the volume times the low price.</li>
          </ul>  
          <p> 
        This dataset is then split into either "Positive" or "Negative" by applying a stop-loss trading strategy to the data-point.
  
        For instance, calling <span class="code">generate_datapoints(0.02, 1e5)</span> and applying a stoploss algorithm that always buys at the open and holds until close, but sells if value decreases below -2%,  results in 3768 'positive' and 5236 'negative' samples. An example of a positive datapoint is the Paxman stock on 2019-03-18 as a result of <a href="https://avanza.se/placera/pressmeddelanden/2019/03/17/paxman-ab-publ-paxman-meddelar-att-nccns-riktlinjer-nu-rekommenderar-skalpkylning-som-en-kategori-2a-behandling-for-patienter-med-brostcancer.html"> this article</a>.
      <img id="paxman" src="ex.png" alt=""></p>


      <h2>Training</h2>
      </section>
      <section><h1>Evaluation</h1>
      </section>
    </div>
  </body>
</html>
